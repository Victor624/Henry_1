<p align=center><img src=https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png><p>

# <h1 align=center> **PROYECTO INDIVIDUAL N¬∫1** </h1>

# <h1 align=center>**`Desarrollado por: Victor de la Merced Vargas`**</h1>


Este proyecto tiene como objetivo utilizar dos conjuntos de datos relacionados con pel√≠culas y actores, llamados "movies_dataset" y "credits". A lo largo del proyecto, abordaremos tareas como ETL, EDA, Machine Learning y el despliegue de una aplicaci√≥n.
 

<hr>  


## Enunciado del Proyecto
Empezaste a trabajar como **`Data Scientist`** en una start-up que provee servicios de agregaci√≥n de plataformas de streaming. El mundo es bello y vas a crear tu primer modelo de ML que soluciona un problema de negocio: un sistema de recomendaci√≥n que a√∫n no ha sido puesto en marcha! 

Vas a sus datos y te das cuenta que la madurez de los mismos es poca (ok, es nula :sob:): Datos anidados, sin transformar, no hay procesos automatizados para la actualizaci√≥n de nuevas pel√≠culas o series, entre otras cosas‚Ä¶.  haciendo tu trabajo imposible :weary:. 

Debes empezar desde 0, haciendo un trabajo r√°pido de **`Data Engineer`** y tener un **`MVP`** (_Minimum Viable Product_) para el cierre del proyecto! Tu cabeza va a explotar ü§Ø, pero al menos sabes cual es, conceptualmente, el camino que debes de seguir :exclamation:. As√≠ que te espantas los miedos y te pones manos a la obra :muscle:



# Diccionario de Carpetas y Archivos En Repositorio


    ‚è©"00 - DATA": "En esta carpeta se guardan los archivos comprimidos de los archivos de Movies_Dataset.csv y 
                    credits.csv" con los cuales vamos a trabajar

    ‚è©"01 - ETL": ""Contiene 5 carpetas que incluyen procesos de transformaci√≥n y limpieza de datos, tratamiento de
	columnas anidadas, sustituci√≥n de valores nulos, revisi√≥n de tipos de datos y c√≥digos para eliminar
	valores repetidos o con formatos distintos a los que corresponden en su columna:
                    01 - Fraccionado DataFrame
                    02 - Desanidado  de Columnas
                    03 - Armado de archivo completo
                    04 - Tratamiento de Datos Nulos
                    05 - Dataframe para operaciones
                    06 - Daraframe data para Funciones",

    ‚è©"02 - EDA ": "Contiene el archivo con el que se realiz√≥ el an√°lisis exploratorio de los datos.",

    ‚è©"03 - DEF": "Genere las funciones necesarias para que nos requirieron en trabajo practico.",

    ‚è©"04 - ML": "Archivo de Machine Learning"

    ‚è©"Henry_1.zip": "Archivo comprimido en formato .zip que contiene la data limpia.",

    ‚è©"main.py": "Archivo que contiene todo el c√≥digo de la API desarrollada con FastAPI.",

    ‚è©"requirements.txt": "Archivo √∫til para realizar el despliegue en Render."

## **Proceso de Trabajo**

A lo largo del proyecto afronte grandes desafios, como entablecer un pensamiento critico y la capacidad de autogestionarme y ser un autodidacta constante, busque, investigue, racionalice, consulte con por varios canales
para encontrar la mejor forma de plasmar todo aprendido.-

Mi repositorio se encuentra establecido de una manera que al descarlo puedan hacer un seguimiento de los procesos
y resultados conseguidos sobre los DataSet Originales.-

En resumen, este proyecto consegui  la autonom√≠a, la organizaci√≥n, la resoluci√≥n de problemas, la comunicaci√≥n escrita, el aprendizaje autodirigido y el pensamiento cr√≠tico. 


# ETL y EDA

## Exploraci√≥n y Limpieza de Datos


La exploracion de datos tuve que desarrolar un sistema donde nada quede librado al azar, donde una simple coma pueda arruinar mi proyecto entonces me enfoque en crear un sistema jerarguico en donde
cada paso que hiba a avanzando y tenia que retroceder para corregir algo, no perdia el camino 
recorrido solo vastaria ir a jupiter en donde tenia que hacer la correcion ejecutaba el cambio y se 
me hiban modificando los archivos consecuentes sin alterar todo lo hecho en el camino.-


Comence creando la carpeta    00 - DATA, ahi guardo los dataset originales de formato zip en donde de ahi empieza todo la limpieza.-

A continuacion cree la carpeta 01 - ETL en donde tambien hice sub-carpetas en donde hice en forma secuencial todo el ETL:


        01 - Fraccionando Daframe: En esta carpeta cree una jupyter para separa los datos para que nececito
        hacerle limpieza, que al darle arranque me divide lo que necesito, como el desanidado del dataset de
        movies y el de Credit, y me duvuelve dos csv que se guardaran en la carpeta siguiente para la siguiente
        transformacion.

        02 - Desanidado de Columnas: En esta carpeta cree dos Jupyter en donde los datos mediantes funciones que
        cree, pude hacer el desanidado en forma casi automatico, otorgandome tiempo y seguiridad a la hora de
        usarlos, de ellos tambien me duvuelve dos csv que se guardaran en la carpeta siguiente para la siguiente
        transformacion.-

        03 - Armado el Completo: En esta carpeta cree un Jupiter donde concatene los datos desencadenados y loS
        uni al dataset original haciendo asi un nuevo dataset, con el que puedo trabajar con los datos nulos Y
        variables inconsistente, lo hice de esta manera para no perder el Indice de todo el daset original.-

        04 - Tratamientos de Datos Nulos: En este Jupiter trate con todos los nulos rellen√© los valores nulos en
        los campos `revenue` y `budget` con el n√∫mero 0. Esta acci√≥n me permiti√≥ manejar adecuadamente los datos
        faltantes y evitar posibles problemas en etapas posteriores del proyecto.
        
        Asimismo, elimin√© los registros con valores nulos en el campo `release date` y asegur√© que todas las
        fechas est√©n en el formato AAAA-mm-dd. Esta transformaci√≥n fue esencial para analizar y comparar de
        manera efectiva las fechas de estreno de las pel√≠culas.
        
        calcul√© un nuevo campo llamado `return`, el cual representa el retorno de inversi√≥n al dividir los campos
        `revenue` y `budget`. En casos donde no hab√≠a datos disponibles para el c√°lculo, asign√© el valor 0.
        
        cre√© una nueva columna llamada `release_year`, la cual me permiti√≥ extraer el a√±o de lanzamiento de cada 
        pel√≠cula.
        Y tambien elimin√© las columnas que no eran relevantes, tales como `video`, `imdb_id`, `adult`
        `original_title`, `poster_path` y `homepage`. Esta acci√≥n me permiti√≥ reducir el ruido en mis datos.-
        
        y tambien me duvuelve un csv que se guardaran en la carpeta siguiente para la siguiente transformacion.

        05 - Dataframe Para Operaciones: En esta carpeta cree un jupyter para separar solo la informacioN
        importante para Hacer el EDA, las Funciones y el Machine Learning y tambien como las otras devuelve.-
        
        06 - Dataset Para Funciones y EDA: Finalmente en esta carpeta guardo los dataset y de esta carpeta se van
        a desprender los archivos necesario para realizar el EDA, Funcione y Machine Learning.-     



## ---------------------------------------------------------------------------------------------------------------


## Diccionario de datos


Haber separado los datos en el ETL en diferentes conjuntos y crear diccionarios de datos para cada uno de ellos ha sido una decisi√≥n clave en mi proyecto individual de Ciencia de Datos. Esta estructura organizativa me ha brindado una serie de beneficios importantes.


A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo data_final.csv:

```python
column_description = {
    ['adult'
     'budget'
     'id'
     'imdb_id'
     'original_language'
     'overview'
     'popularity'
     'release_date'
     'revenue'
     'runtime'
     'status'
     'tagline',
     'title'
     'vote_average'
     'vote_count'
     'id.1'
     'Actor'
     'Director'
     'id_belong'
     'name_belong'
     'name_genres'
     'id_genres'
     'name_Spoken'
     'iso_639_1_Spoken'
     'id_prod'
     'name_prod'
     'iso_3166_1_Count'
     'name_Count
     'release_year'
     'Return'
}

```

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo Funcio.csv:

```python
column_description = {
    'title'
    'original_language'
    'release_year'
    'name_belong'
    'runtime',
    'name_prod'
    'Return'
    'name_Count'
    'release_date'
    'revenue'
    'Return'
    'Director'
}

```

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo Machine.csv:

```python
column_description = {
    'title'
    "name_genres"
    'popularity''
}
```


## s---------------------------------------------------------------------------------------------------------------

## An√°lisis Exploratorio de Datos (EDA) Carpeta 02 - EDA

Despu√©s de completar las tareas de limpieza de datos, realic√© un an√°lisis exploratorio exhaustivo utilizando t√©cnicas estad√≠sticas y visualizaciones. El archivo donde se llev√≥ a cabo este an√°lisis se encuentra en la carpeta "data_a_explorar", en un notebook que contiene las gr√°ficas de exploraci√≥n.

Durante el an√°lisis exploratorio, utilic√© las siguientes librer√≠as: pandas, numpy, matplotlib.pyplot y seaborn. Estas herramientas me permitieron realizar diversas actividades, como calcular el porcentaje de valores faltantes en cada columna, filtrar las columnas con valores faltantes y examinar variables num√©ricas como "popularity", "vote_average", "vote_count", "runtime", "budget", "revenue" y "return".

Tambi√©n realic√© c√°lculos de estad√≠sticas descriptivas, como la media, mediana, desviaci√≥n est√°ndar y percentiles, para comprender la distribuci√≥n de los datos. Gener√© histogramas y gr√°ficos de caja para visualizar las variables num√©ricas, as√≠ como un gr√°fico de dispersi√≥n para analizar la relaci√≥n entre los ingresos y presupuestos de las pel√≠culas.

Adem√°s, realic√© diversas visualizaciones para explorar la distribuci√≥n de pel√≠culas por mes y a√±o de lanzamiento, los pa√≠ses con mayor producci√≥n cinematogr√°fica, los g√©neros m√°s populares a lo largo del tiempo, entre otros an√°lisis. Tambi√©n gener√© un mapa de calor para examinar las relaciones entre las variables.

Este an√°lisis exploratorio de datos fue fundamental para obtener conocimientos valiosos que nos ayudaron a comprender mejor el conjunto de datos y a tomar decisiones informadas en etapas posteriores del proyecto. Nos permiti√≥ descubrir patrones, identificar outliers y obtener una comprensi√≥n m√°s profunda de las caracter√≠sticas de las pel√≠culas y su √©xito financiero.

# Funciones

## Desarrollo de Funciones (DEF) carpeta 03 - DEF


Deben crear 6 funciones para los endpoints que se consumir√°n en la API, recuerden que deben tener un decorador por cada una .

                def peliculas_idioma( Idioma: str ): Esta Funcion devuelve el idioma Idioma solicitado y la
    cantidad de  peliculas hechas con ese idioma:

                    Ejemplo de retorno: X cantidad de pel√≠culas fueron estrenadas en idioma

                def peliculas_duracion( Pelicula: str ): Se ingresa una pelicula. Debe devolver la duracion y el a√±o. 
                    Ejemplo de retorno: X . Duraci√≥n: x. A√±o: xx

                def franquicia( Franquicia: str ): Se ingresa la franquicia, retornando la cantidad de peliculas
    ganancia total y promedio
                    Ejemplo de retorno: La franquicia X posee X peliculas, una ganancia total de x y una ganancia promedio de xx

                def peliculas_pais( Pais: str ): Se ingresa un pa√≠s , retornando la cantidad de peliculas
    producidas en el mismo.

                    Ejemplo de retorno: Se produjeron X pel√≠culas en el pa√≠s X

                def productoras_exitosas( Productora: str ): Se ingresa la productora, entregandote el revunue total y la cantidad de peliculas que realizo.

                    Ejemplo de retorno: La productora X ha tenido un revenue de x

                def get_director( nombre_director ): Se ingresa el nombre de un director que se encuentre dentro
    de un dataset debiendo devolver el √©xito del mismo medido a trav√©s del retorno. Adem√°s, deber√° devolver el 
    nombre de cada pel√≠cula con la fecha de lanzamiento, retorno individual, costo y ganancia de la misma, en 
    formato lista.



# Sistema de Recomendaci√≥n

## Desarrollo de Modelos de Machine Learning (ML) carpeta 04 - ML


Para el desarrollo del sistema de recomendaci√≥n utile es elm√©todo de los k vecinos m√°s cercanos 
(en ingl√©s: k-nearest neighbors, abreviadok-nn) es un m√©todo de clasificaci√≥n supervisada (Aprendizaje,
 estimaci√≥n basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la funci√≥n de densidad 

Este es un m√©todo de clasificaci√≥n no param√©trico, que estima el valor de la funci√≥n de densidad de probabilidad 
o directamente la probabilidad a posteriori de que un elemento x pertenezca a la clase 

 a partir de la informaci√≥n proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposici√≥n acerca de la distribuci√≥n de las variables predictoras.

 En el reconocimiento de patrones, el algoritmo k-nn es usado como m√©todo de clasificaci√≥n de objetos (elementos) 
 basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos. 

k-nn es un tipo de aprendizaje vago (lazy learning), donde la funci√≥n se aproxima solo localmente y todo el
 c√≥mputo es diferido a la clasificaci√≥n. La normalizaci√≥n de datos puede mejorar considerablemente la exactitud 
 del algoritmo.

## ---------------------------------------------------------------------------------------------------------------

# API con FASTAPI

## Desarrollo de la API

La API se desarroll√≥ utilizando FastAPI, un framework web de Python que nos permite crear servicios web de manera r√°pida y eficiente. A continuaci√≥n, se mencionan las librer√≠as y frameworks utilizados en la creaci√≥n de la API, junto con una breve descripci√≥n de su funci√≥n y uso en el proyecto:

- `FastAPI`: FastAPI es un framework web de alto rendimiento basado en Python. Se utiliz√≥ para crear y gestionar la API, proporcionando rutas y controladores para las diferentes funciones y endpoints.
- `pandas`: pandas es una librer√≠a de Python ampliamente utilizada para la manipulaci√≥n y an√°lisis de datos. Se utiliz√≥ para cargar y procesar los conjuntos de datos, permitiendo realizar consultas y realizar operaciones sobre ellos.
- `zipfile`: zipfile es una librer√≠a de Python que permite trabajar con archivos comprimidos en formato ZIP. Se utiliz√≥ para descomprimir archivos ZIP que conten√≠an los conjuntos de datos necesarios para la API.
- `sklearn.neighbors`: sklearn.neighbors es un m√≥dulo de la librer√≠a scikit-learn que contiene algoritmos de vecinos m√°s cercanos (K-Nearest Neighbors). Se utiliz√≥ para implementar funcionalidades relacionadas con el sistema de recomendaci√≥n, como encontrar vecinos m√°s cercanos basados en caracter√≠sticas similares.

Cada una de estas librer√≠as y frameworks desempe√±√≥ un papel crucial en el desarrollo de la API y permiti√≥ implementar diferentes funcionalidades, desde el procesamiento de datos hasta la creaci√≥n de modelos de Machine Learning para el sistema de recomendaci√≥n. Su uso combinado proporcion√≥ las herramientas necesarias para construir una API robusta y funcional.



# Deployment

## Despliegue (Deployment)

Para el despliegue de la API en Render, se cre√≥ un archivo llamado `requirements.txt`. Este archivo es utilizado para especificar las dependencias y las versiones exactas de las librer√≠as que son necesarias para que la API funcione correctamente.

Render utiliza el archivo `requirements.txt` para instalar autom√°ticamente las dependencias especificadas en el entorno de ejecuci√≥n de la aplicaci√≥n. Al incluir las dependencias y las versiones adecuadas en este archivo, se asegura que la API pueda ejecutarse sin problemas en Render, con todas las bibliotecas necesarias correctamente instaladas.

Cada l√≠nea del archivo especifica el nombre de una librer√≠a seguido de `==` y la versi√≥n requerida. Pueden incluirse tantas l√≠neas como sean necesarias para todas las dependencias de la API.

Una vez que el archivo `requirements.txt` est√° correctamente configurado, Render utilizar√° esta informaci√≥n para instalar autom√°ticamente las librer√≠as necesarias durante el proceso de despliegue de la API. Esto asegura que todas las dependencias est√©n disponibles en el entorno de ejecuci√≥n de la aplicaci√≥n en Render.

<a href="https://trabajo-vargas-victor.onrender.com/docs#" target="_blank">Mi Aplicacion</a>

<a href="https://trabajo-vargas-victor.onrender.com/" target="_blank">Mi Aplicacion</a>


