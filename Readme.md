<p align=center><img src=https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png><p>

# <h1 align=center> **PROYECTO INDIVIDUAL N¬∫1** </h1>

# <h1 align=center>**`Desarrollado por: Victor de la Merced Vargas`**</h1>


Este proyecto tiene como objetivo utilizar dos conjuntos de datos relacionados con pel√≠culas y actores, llamados "movies_dataset" y "credits". A lo largo del proyecto, abordaremos tareas como ETL, EDA, Machine Learning y el despliegue de una aplicaci√≥n.
 

<hr>  


## Enunciado del Proyecto
Empezaste a trabajar como **`Data Scientist`** en una start-up que provee servicios de agregaci√≥n de plataformas de streaming. El mundo es bello y vas a crear tu primer modelo de ML que soluciona un problema de negocio: un sistema de recomendaci√≥n que a√∫n no ha sido puesto en marcha! 

Vas a sus datos y te das cuenta que la madurez de los mismos es poca (ok, es nula :sob:): Datos anidados, sin transformar, no hay procesos automatizados para la actualizaci√≥n de nuevas pel√≠culas o series, entre otras cosas‚Ä¶.  haciendo tu trabajo imposible :weary:. 

Debes empezar desde 0, haciendo un trabajo r√°pido de **`Data Engineer`** y tener un **`MVP`** (_Minimum Viable Product_) para el cierre del proyecto! Tu cabeza va a explotar ü§Ø, pero al menos sabes cual es, conceptualmente, el camino que debes de seguir :exclamation:. As√≠ que te espantas los miedos y te pones manos a la obra :muscle:



# Diccionario de Carpetas y Archivos En Repositorio


    ‚è©"data": "En esta carpeta se guardan los archivos descomprimidos del archivo data_1.0.2",

    ‚è©"EDA": "Esta carpeta contiene un archivo en formato .ipynb que se utiliz√≥ para realizar el an√°lisis
	exploratorio de los datos.",

    ‚è©"EDA/data_a_explorar": "Contiene el archivo con el que se realiz√≥ el an√°lisis exploratorio de los datos.",

    ‚è©"ETL": "Contiene 9 carpetas que incluyen procesos de transformaci√≥n y limpieza de datos, tratamiento de
	columnas anidadas, sustituci√≥n de valores nulos, revisi√≥n de tipos de datos y c√≥digos para eliminar
	valores repetidos o con formatos distintos a los que corresponden en su columna.",

    ‚è©"ETL/1-Arquitectura_Nuevos_DataSets": "Contiene un archivo en formato .ipynb en el que se realiz√≥ la
	separaci√≥n del dataset original y se crearon nuevos datasets. Para m√°s detalles, revisar el
	diccionario de datos.",

    ‚è©"ETL/6-DataSets_Final_movies_dataset": "Contiene un archivo en formato .zip en el que se encuentran
	comprimidos los archivos limpios.",

    ‚è©"ETL/7-Datasets_original": "Contiene los archivos originales comprimidos en un archivo .zip.",

    ‚è©"ETL/9-Proceso_ETL_credits": "Contiene dos archivos en formato .ipynb donde se realiza el proceso
	ETL del archivo credits.csv.",

    ‚è©"Machine_Learning/Pruebas": "Contiene un archivo en formato .ipynb en el que se desarroll√≥ la funci√≥n
	para crear un modelo de aprendizaje utilizando el m√©todo 'vecinos m√°s cercanos'.",

    ‚è©"Scripts": "Contiene archivos relacionados con el entorno virtual"

    ‚è©"data_1.0.2.zip": "Archivo comprimido en formato .zip que contiene la data limpia.",

    ‚è©"index.html": "Archivo con el c√≥digo en HTML, JavaScript y CSS que se utiliz√≥ para crear el Frontend.",

    ‚è©"main.py": "Archivo que contiene todo el c√≥digo de la API desarrollada con FastAPI.",

    ‚è©"requirements.txt": "Archivo √∫til para realizar el despliegue en Render."

## **Proceso de Trabajo**



A lo largo del proyecto, enfrent√© diversos desaf√≠os que requer√≠an de un pensamiento cr√≠tico y la capacidad de aprender de manera autodirigida. Investigu√© soluciones a problemas t√©cnicos y adquir√≠ nuevos conocimientos en el √°rea de Ingenier√≠a de Datos y Machine Learning para poder desarrollar y aplicar modelos de aprendizaje autom√°tico de forma efectiva.

En resumen, este proyecto me permiti√≥ fortalecer habilidades como la autonom√≠a, la organizaci√≥n, la resoluci√≥n de problemas, la comunicaci√≥n escrita, el aprendizaje autodirigido y el pensamiento cr√≠tico. Estas habilidades resultaron fundamentales para lograr el √©xito en el desarrollo de este proyecto y estoy entusiasmado/a de seguir cultiv√°ndolas en futuras oportunidades.

# ETL y EDA

## Exploraci√≥n y Limpieza de Datos

Despu√©s de realizar el proceso de Extracci√≥n, Transformaci√≥n y Carga (ETL) en mi proyecto de Ciencia de Datos, me encontr√© en una etapa emocionante donde cre√© nuevos conjuntos de datos con el objetivo de obtener una visi√≥n a√∫n m√°s clara y enriquecedora de mis datos.

Durante la exploraci√≥n y limpieza de los datos iniciales, desanid√© campos como `belongs_to_collection` y `production_companies`, lo cual me permiti√≥ extraer informaci√≥n relevante que posteriormente utilic√© en consultas de la API. Esta nueva informaci√≥n me brind√≥ una visi√≥n m√°s detallada sobre las colecciones a las que pertenecen las pel√≠culas y las compa√±√≠as de producci√≥n involucradas, enriqueciendo as√≠ mi an√°lisis.

Adem√°s, para garantizar la integridad de mi conjunto de datos, rellen√© los valores nulos en los campos `revenue` y `budget` con el n√∫mero 0. Esta acci√≥n me permiti√≥ manejar adecuadamente los datos faltantes y evitar posibles problemas en etapas posteriores del proyecto.

Asimismo, elimin√© los registros con valores nulos en el campo `release date` y asegur√© que todas las fechas est√©n en el formato AAAA-mm-dd. Esta transformaci√≥n fue esencial para analizar y comparar de manera efectiva las fechas de estreno de las pel√≠culas.

Para ampliar mis posibilidades de an√°lisis, cre√© una nueva columna llamada `release_year`, la cual me permiti√≥ extraer el a√±o de lanzamiento de cada pel√≠cula. Esta adici√≥n me brind√≥ la capacidad de realizar an√°lisis basados en a√±os y observar las tendencias a lo largo del tiempo, brindando una perspectiva temporal valiosa.

Con el objetivo de evaluar el rendimiento financiero de las pel√≠culas, calcul√© un nuevo campo llamado `return`, el cual representa el retorno de inversi√≥n al dividir los campos `revenue` y `budget`. En casos donde no hab√≠a datos disponibles para el c√°lculo, asign√© el valor 0. Esta m√©trica proporcion√≥ informaci√≥n valiosa sobre el rendimiento financiero de las pel√≠culas, permiti√©ndome tomar decisiones informadas basadas en datos.

Por √∫ltimo, para simplificar mi conjunto de datos y enfocarme en las variables m√°s significativas para mi proyecto, elimin√© las columnas que no eran relevantes, tales como `video`, `imdb_id`, `adult`, `original_title`, `poster_path` y `homepage`. Esta acci√≥n me permiti√≥ reducir el ruido en mis datos y concentrarme en las caracter√≠sticas clave que impulsar√≠an mi an√°lisis y resultados.

En resumen, despu√©s de completar el proceso de ETL, cre√© nuevos conjuntos de datos que me proporcionaron una visi√≥n m√°s amplia y mejorada de los datos, prepar√°ndome para las etapas posteriores del proyecto y brind√°ndome una base s√≥lida para realizar an√°lisis m√°s profundos y reveladores.

## ---------------------------------------------------------------------------------------------------------------


## Diccionario de datos

Haber separado los datos en diferentes conjuntos y crear diccionarios de datos para cada uno de ellos ha sido una decisi√≥n clave en mi proyecto individual de Ciencia de Datos. Esta estructura organizativa me ha brindado una serie de beneficios importantes.

Al separar los datos en conjuntos espec√≠ficos, como datasets_final.csv, ML_data.csv, cast_data.csv, crew_data.csv y movie_genres.csv, he logrado una mayor modularidad y claridad en la estructura de mi proyecto. Cada archivo representa un aspecto distinto de los datos y me permite trabajar en cada uno de ellos de manera independiente y centrada en sus caracter√≠sticas √∫nicas.

El archivo ML_data.csv desempe√±√≥ un papel fundamental en mi proyecto de Machine Learning. Las columnas disponibles en este conjunto de datos me proporcionaron la informaci√≥n necesaria para realizar una predicci√≥n utilizando el algoritmo de "Vecinos m√°s Cercanos" (K-Nearest Neighbors) en el contexto de Machine Learning. Utilizando estas caracter√≠sticas, pude entrenar el modelo y hacer predicciones precisas sobre la popularidad de las pel√≠culas.

El diccionario de datos asociado a datasets_final.csv proporciona descripciones detalladas de cada columna en este conjunto de datos, lo cual es fundamental para comprender la informaci√≥n contenida en √©l. Al conocer el prop√≥sito y el significado de cada columna, puedo tomar decisiones m√°s informadas y realizar an√°lisis m√°s precisos y relevantes.

Del mismo modo, los diccionarios de datos asociados a cast_data.csv, crew_data.csv y movie_genres.csv me permiten entender r√°pidamente las columnas presentes en cada conjunto de datos y su significado. Esto facilita la manipulaci√≥n, el procesamiento y la interpretaci√≥n de los datos en cada contexto espec√≠fico.

En resumen, la separaci√≥n de los datos en diferentes conjuntos y la creaci√≥n de diccionarios de datos espec√≠ficos para cada uno de ellos ha sido una elecci√≥n estrat√©gica en mi proyecto individual de Ciencia de Datos. Esta estructura modular y descriptiva me ha permitido realizar an√°lisis m√°s profundos, utilizar el conjunto de datos ML_data.csv para el entrenamiento de un modelo de Machine Learning y realizar predicciones precisas utilizando el algoritmo de "Vecinos m√°s Cercanos" (K-Nearest Neighbors). Adem√°s, me ha proporcionado una visi√≥n m√°s completa de los diferentes aspectos de las pel√≠culas, como su informaci√≥n general, elenco, equipo de producci√≥n y g√©neros.

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo datasets_final.csv:

```python
column_description = {
    'id': 'ID de la pel√≠cula',
    'title': 'T√≠tulo de la pel√≠cula',
    'overview': 'Descripci√≥n de la pel√≠cula',
    'popularity': 'Popularidad de la pel√≠cula',
    'vote_average': 'Promedio de votos de la pel√≠cula',
    'vote_count': 'N√∫mero de votos de la pel√≠cula',
    'status': 'Estado de la pel√≠cula',
    'original_language': 'Idioma original de la pel√≠cula',
    'runtime': 'Duraci√≥n de la pel√≠cula en minutos',
    'budget': 'Presupuesto de la pel√≠cula',
    'revenue': 'Ingresos generados por la pel√≠cula',
    'tagline': 'Lema de la pel√≠cula',
    'id_btc': 'ID de la pel√≠cula en BTC',
    'name_btc': 'Nombre de la pel√≠cula en BTC',
    'poster_btc': 'URL del p√≥ster de la pel√≠cula en BTC',
    'backdrop_btc': 'URL del fondo de la pel√≠cula en BTC',
    'iso_639_1': 'C√≥digo ISO 639-1 del idioma',
    'language_name': 'Nombre del idioma',
    'release_year': 'A√±o de lanzamiento de la pel√≠cula',
    'return': 'Relaci√≥n entre ingresos y presupuesto de la pel√≠cula',
    'companies_id': 'ID de las compa√±√≠as de producci√≥n',
    'companies_name': 'Nombres de las compa√±√≠as de producci√≥n',
    'countries_iso': 'C√≥digos ISO de los pa√≠ses de producci√≥n',
    'countries_name': 'Nombres de los pa√≠ses de producci√≥n',
    'release_date': 'Fecha de lanzamiento de la pel√≠cula',
    'month_time': 'Mes en el que se cre√≥ la pel√≠cula',
    'day_time': 'D√≠a en el que se cre√≥ la pel√≠cula'
}

```

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo ML_data.csv:

```python
column_description = {
    'id': 'ID de la pel√≠cula',
    'title': 'T√≠tulo de la pel√≠cula',
    'genero': 'G√©nero de la pel√≠cula',
    'popularity': 'Popularidad de la pel√≠cula'
}

```

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo cast_data.csv:

```python
column_description = {
    'id': 'ID de la pel√≠cula',
    'cast': 'Elenco de la pel√≠cula en formato JSON'
}
```

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo crew_data.csv:

```python
column_description = {
    'id': 'ID de la pel√≠cula',
    'crew_credit_id': 'ID de cr√©dito del equipo de producci√≥n',
    'crew_department': 'Departamento del equipo de producci√≥n',
    'crew_gender': 'Actividad en el equipo de producci√≥n',
    'crew_id': 'ID del miembro del equipo de producci√≥n',
    'crew_job': 'Trabajo del miembro del equipo de producci√≥n',
    'crew_name': 'Nombre del miembro del equipo de producci√≥n',
    'crew_profile_path': 'Ruta del perfil del miembro del equipo de producci√≥n'
}

```

A continuaci√≥n se muestra un diccionario que describe cada columna en el conjunto de datos del archivo movie_genres.csv:

```python
column_description = {
    'id': 'ID de la pel√≠cula',
    'id_genres': 'ID de g√©neros asociados a la pel√≠cula',
    'genero': 'G√©neros de la pel√≠cula'
}

```
## s---------------------------------------------------------------------------------------------------------------

## An√°lisis Exploratorio de Datos (EDA)

Despu√©s de completar las tareas de limpieza de datos, realic√© un an√°lisis exploratorio exhaustivo utilizando t√©cnicas estad√≠sticas y visualizaciones. El archivo donde se llev√≥ a cabo este an√°lisis se encuentra en la carpeta "data_a_explorar", en un notebook que contiene las gr√°ficas de exploraci√≥n.

Durante el an√°lisis exploratorio, utilic√© las siguientes librer√≠as: pandas, numpy, matplotlib.pyplot y seaborn. Estas herramientas me permitieron realizar diversas actividades, como calcular el porcentaje de valores faltantes en cada columna, filtrar las columnas con valores faltantes y examinar variables num√©ricas como "popularity", "vote_average", "vote_count", "runtime", "budget", "revenue" y "return".

Tambi√©n realic√© c√°lculos de estad√≠sticas descriptivas, como la media, mediana, desviaci√≥n est√°ndar y percentiles, para comprender la distribuci√≥n de los datos. Gener√© histogramas y gr√°ficos de caja para visualizar las variables num√©ricas, as√≠ como un gr√°fico de dispersi√≥n para analizar la relaci√≥n entre los ingresos y presupuestos de las pel√≠culas.

Adem√°s, realic√© diversas visualizaciones para explorar la distribuci√≥n de pel√≠culas por mes y a√±o de lanzamiento, los pa√≠ses con mayor producci√≥n cinematogr√°fica, los g√©neros m√°s populares a lo largo del tiempo, entre otros an√°lisis. Tambi√©n gener√© un mapa de calor para examinar las relaciones entre las variables.

Este an√°lisis exploratorio de datos fue fundamental para obtener conocimientos valiosos que nos ayudaron a comprender mejor el conjunto de datos y a tomar decisiones informadas en etapas posteriores del proyecto. Nos permiti√≥ descubrir patrones, identificar outliers y obtener una comprensi√≥n m√°s profunda de las caracter√≠sticas de las pel√≠culas y su √©xito financiero.

# Sistema de Recomendaci√≥n

## Desarrollo de Modelos de Machine Learning

Adem√°s del an√°lisis exploratorio de datos, implemente un modelo de Machine Learning para resolver el siguiente desaf√≠o:

* Sistema de recomendaci√≥n: Utilizamos t√©cnicas de filtrado colaborativo y/o basado en contenido para construir un sistema de recomendaci√≥n de pel√≠culas personalizadas.
* Esto permiti√≥ a los usuarios descubrir nuevas pel√≠culas en funci√≥n de sus preferencias. Mediante la API, los usuarios pueden ingresar el nombre de una pel√≠cula y el endpoint correspondiente les proporcionar√° 5 recomendaciones basadas en sus caracter√≠sticas y en las preferencias de otros usuarios con gustos similares. Esto mejora la experiencia del usuario al ofrecer sugerencias relevantes y personalizadas para su disfrute cinematogr√°fico.

## An√°lisis en el que me bas√© para utilizar el modelo de aprendizaje autom√°tico "k vecinos m√°s cercanos"

Para el desarrollo del sistema de recomendaci√≥n, se nos proporcion√≥ un enunciado. Cito fracci√≥n textual del enunciado: "Este consiste en recomendar pel√≠culas a los usuarios bas√°ndose en pel√≠culas similares, por lo que se debe encontrar la similitud de puntuaci√≥n entre esa pel√≠cula y el resto de pel√≠culas". El enunciado me pide que encuentre la similitud de puntuaci√≥n entre una pel√≠cula y las dem√°s pel√≠culas, y naturalmente, lo primero que me viene a la mente para crear el sistema de recomendaci√≥n es utilizar la funci√≥n "cosine_similarity".

La funci√≥n "cosine_similarity" es una medida com√∫nmente utilizada en el campo de la recuperaci√≥n de informaci√≥n y la miner√≠a de texto para evaluar la similitud entre dos vectores de caracter√≠sticas. En particular, la similitud del coseno se utiliza con mayor frecuencia para comparar la similitud entre vectores que representan documentos o textos.

Sin embargo, la funci√≥n "cosine_similarity" tiene algunas limitaciones, y una de las que m√°s llam√≥ mi atenci√≥n es que opera con vectores de caracter√≠sticas num√©ricas. Por lo tanto, si los vectores contienen valores no num√©ricos o datos no estructurados, es posible que la funci√≥n no sea aplicable directamente. Esta limitaci√≥n capt√≥ mi atenci√≥n, pero tambi√©n hay otras consideraciones, como el espacio vectorial, la longitud y dimensionalidad de los vectores, la sensibilidad a la magnitud, entre otras.

Para poder realizar una mejor predicci√≥n en el modelo, no solo pod√≠a basarme en la puntuaci√≥n de la pel√≠cula, ya que existen otros factores importantes y de mucha m√°s relevancia que la puntuaci√≥n. Un ejemplo de ello es el g√©nero de la pel√≠cula. En un caso concreto, puedo mencionar dos pel√≠culas con altas puntuaciones, pero de g√©neros totalmente distintos.

## ---------------------------------------------------------------------------------------------------------------

# API con FASTAPI

## Desarrollo de la API

La API se desarroll√≥ utilizando FastAPI, un framework web de Python que nos permite crear servicios web de manera r√°pida y eficiente. A continuaci√≥n, se mencionan las librer√≠as y frameworks utilizados en la creaci√≥n de la API, junto con una breve descripci√≥n de su funci√≥n y uso en el proyecto:

- `FastAPI`: FastAPI es un framework web de alto rendimiento basado en Python. Se utiliz√≥ para crear y gestionar la API, proporcionando rutas y controladores para las diferentes funciones y endpoints.
- `pandas`: pandas es una librer√≠a de Python ampliamente utilizada para la manipulaci√≥n y an√°lisis de datos. Se utiliz√≥ para cargar y procesar los conjuntos de datos, permitiendo realizar consultas y realizar operaciones sobre ellos.
- `zipfile`: zipfile es una librer√≠a de Python que permite trabajar con archivos comprimidos en formato ZIP. Se utiliz√≥ para descomprimir archivos ZIP que conten√≠an los conjuntos de datos necesarios para la API.
- `sklearn.neighbors`: sklearn.neighbors es un m√≥dulo de la librer√≠a scikit-learn que contiene algoritmos de vecinos m√°s cercanos (K-Nearest Neighbors). Se utiliz√≥ para implementar funcionalidades relacionadas con el sistema de recomendaci√≥n, como encontrar vecinos m√°s cercanos basados en caracter√≠sticas similares.

Cada una de estas librer√≠as y frameworks desempe√±√≥ un papel crucial en el desarrollo de la API y permiti√≥ implementar diferentes funcionalidades, desde el procesamiento de datos hasta la creaci√≥n de modelos de Machine Learning para el sistema de recomendaci√≥n. Su uso combinado proporcion√≥ las herramientas necesarias para construir una API robusta y funcional.

Para instalar las librer√≠as y frameworks mencionados, puedes utilizar los siguientes comandos:

1. FastAPI:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs">pip install fastapi
</code></div></div></pre>

2. pandas:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs">pip install pandas
</code></div></div></pre>

3. zipfile:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>css</span><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs language-css">No es necesario instalar esta librer√≠a, ya que es parte de la biblioteca est√°ndar de Python y viene incluida por defecto.
</code></div></div></pre>

4. scikit-learn:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs">pip install scikit-learn
</code></div></div></pre>

Una vez que hayas ejecutado estos comandos, tendr√°s instaladas las librer√≠as y frameworks necesarios para ejecutar la API y utilizar sus funcionalidades. Es recomendable utilizar un entorno virtual para mantener un ambiente de desarrollo aislado y evitar conflictos entre las dependencias de diferentes proyectos.

Para crear y activar un entorno virtual con Virtualenv, puedes utilizar los siguientes comandos:

1. Instalaci√≥n de Virtualenv:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs">pip install virtualenv
</code></div></div></pre>

2. Creaci√≥n de un nuevo entorno virtual:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs">virtualenv nombre_del_entorno
</code></div></div></pre>

3. Activaci√≥n del entorno virtual:

* En Windows:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs">nombre_del_entorno\Scripts\activate
</code></div></div></pre>

* En macOS/Linux:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>bash</span><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs language-bash">source nombre_del_entorno/bin/activate
</code></div></div></pre>

Una vez que hayas activado el entorno virtual, puedes proceder a instalar las librer√≠as y frameworks mencionados utilizando los comandos que proporcion√© anteriormente. Esto asegurar√° que las dependencias se instalen dentro del entorno virtual y no afecten a tu entorno de desarrollo principal.

Recuerda que, para desactivar el entorno virtual, simplemente ejecuta el comando `deactivate`.

# Deployment

## Despliegue (Deployment)

Para el despliegue de la API en Render, se cre√≥ un archivo llamado `requirements.txt`. Este archivo es utilizado para especificar las dependencias y las versiones exactas de las librer√≠as que son necesarias para que la API funcione correctamente.

Render utiliza el archivo `requirements.txt` para instalar autom√°ticamente las dependencias especificadas en el entorno de ejecuci√≥n de la aplicaci√≥n. Al incluir las dependencias y las versiones adecuadas en este archivo, se asegura que la API pueda ejecutarse sin problemas en Render, con todas las bibliotecas necesarias correctamente instaladas.

El contenido del archivo `requirements.txt` sigue el siguiente formato:

<pre><div class="bg-black rounded-md mb-4"><div class="flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans justify-between rounded-t-md"><span>makefile</span><button class="flex ml-auto gap-2"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path><rect x="8" y="2" width="8" height="4" rx="1" ry="1"></rect></svg>Copy code</button></div><div class="p-4 overflow-y-auto"><code class="!whitespace-pre hljs language-makefile">libreria1==version1
libreria2==version2
...
</code></div></div></pre>

Cada l√≠nea del archivo especifica el nombre de una librer√≠a seguido de `==` y la versi√≥n requerida. Pueden incluirse tantas l√≠neas como sean necesarias para todas las dependencias de la API.

Una vez que el archivo `requirements.txt` est√° correctamente configurado, Render utilizar√° esta informaci√≥n para instalar autom√°ticamente las librer√≠as necesarias durante el proceso de despliegue de la API. Esto asegura que todas las dependencias est√©n disponibles en el entorno de ejecuci√≥n de la aplicaci√≥n en Render.

<a href="https://jorgeluisgarcia-pt01.onrender.com/" target="_blank">Mi Aplicacion</a>


